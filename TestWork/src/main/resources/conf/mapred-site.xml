<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<!--  non-HA
<property>
    <name>mapred.job.tracker</name>
    <value>compute-13-15.local:9002</value>
</property>
-->

<!-- jobtracker ha conf -->
<property>
    <name>mapred.job.tracker</name>
    <value>logicaljt</value> 
    <!-- host:port string is replaced with a logical name -->
</property>

<property>
    <name>mapred.jobtrackers.logicaljt</name>
    <value>jt1,jt2</value>
    <description>Comma-separated list of JobTracker IDs.</description>
</property>

<property>
    <name>mapred.jobtracker.rpc-address.logicaljt.jt1</name> 
    <!-- RPC address for jt1 -->
    <value>edc01:8121</value>
</property>

<property>
    <name>mapred.jobtracker.rpc-address.logicaljt.jt2</name> 
    <!-- RPC address for jt2 -->
    <value>edc02:8122</value>
</property>

<property>
    <name>mapred.job.tracker.http.address.logicaljt.jt1</name> 
    <!-- HTTP bind address for jt1 -->
    <value>0.0.0.0:50030</value>
</property>

<property>
    <name>mapred.job.tracker.http.address.logicaljt.jt2</name> 
    <!-- HTTP bind address for jt2 -->
    <value>0.0.0.0:50030</value>
</property>

<property>
    <name>mapred.ha.jobtracker.rpc-address.logicaljt.jt1</name> 
    <!-- RPC address for jt1 HA daemon -->
    <value>edc01:8123</value>
</property>

<property>
    <name>mapred.ha.jobtracker.rpc-address.logicaljt.jt2</name> 
    <!-- RPC address for jt2 HA daemon -->
    <value>edc02:8124</value>
</property>

<property>
    <name>mapred.ha.jobtracker.http-redirect-address.logicaljt.jt1</name> 
    <!-- HTTP redirect address for jt1 -->
    <value>edc01:50030</value>
</property>

<property>
    <name>mapred.ha.jobtracker.http-redirect-address.logicaljt.jt2</name> 
    <!-- HTTP redirect address for jt2 -->
    <value>edc02:50031</value>
</property>

<property>
    <name>mapred.jobtracker.restart.recover</name>
    <value>true</value>
</property>

<property>
    <name>mapred.job.tracker.persist.jobstatus.active</name>
    <value>true</value>
</property>

<property>
    <name>mapred.job.tracker.persist.jobstatus.hours</name>
    <value>1</value>
</property>

<property>
    <name>mapred.job.tracker.persist.jobstatus.dir</name>
    <value>/jobtracker/jobsInfo</value>
</property>

<property>
    <name>mapred.client.failover.proxy.provider.logicaljt</name>
    <value>org.apache.hadoop.mapred.ConfiguredFailoverProxyProvider</value>
</property>

<property>
    <name>mapred.client.failover.max.attempts</name>
    <value>15</value>
</property>

<property>
    <name>mapred.client.failover.sleep.base.millis</name>
    <value>500</value>
</property>

<property>
    <name>mapred.client.failover.sleep.max.millis</name>
    <value>1500</value>  
</property>

<property>
    <name>mapred.client.failover.connection.retries</name>
    <value>0</value>  
</property>

<property>
    <name>mapred.client.failover.connection.retries.on.timeouts</name>
    <value>0</value>  
</property>

<property>
    <name>mapred.ha.fencing.methods</name>
    <!--value>shell(/home/bc/cdh4/hadoop-2.0.0-cdh4.3.0/bin-mapreduce1/restart-tts.sh)</value-->
    <value>shell(/bin/true)</value>
</property>

<property>
    <name>mapred.ha.automatic-failover.enabled</name>
    <value>true</value>
</property>

<property>
    <name>mapred.ha.zkfc.port</name>
    <value>8018</value> 
    <!-- Pick a different port for each failover controller when running one machine -->
</property>

<property>
    <name>ha.zookeeper.quorum</name>
    <value>edc01:2181,edc02:2181,edc03:2181</value>
</property>

<!-- jobtracker ha conf END -->


<!-- common conf -->
<property>
  <name>mapred.task.tracker.http.address</name>
  <value>0.0.0.0:51060</value>
  <description>
    The task tracker http server address and port.
    If the port is 0 then the server will start on a free port.
  </description>
</property>

<property>
  <name>mapred.task.tracker.report.address</name>
  <value>127.0.0.1:0</value> 
  <description>The interface and port that task tracker server listens on.
  Since it is only connected to by the tasks, it uses the local interface.
  EXPERT ONLY. Should only be changed if your host does not have the loopback
  interface.</description>
</property>


<property>
  <name>mapred.local.dir</name>
    <value>/data/hadoop/data/mrlocal</value>
</property>

<property>
  <name>mapred.map.tasks</name>
  <value>1</value>
  <description>The default number of map tasks per job.
  Ignored when mapred.job.tracker is "local".
  </description>
</property>

<property>
  <name>mapred.reduce.tasks</name>
  <value>4</value>
  <description>The default number of reduce tasks per job. Typically set to 99%
  of the cluster's reduce capacity, so that if a node fails the reduces can
  still be executed in a single wave.
  Ignored when mapred.job.tracker is "local".
  </description>
</property>

<property>
  <name>mapred.tasktracker.map.tasks.maximum</name>
  <value>2</value>
  <description>The maximum number of map tasks that will be run
  simultaneously by a task tracker.
  </description>
</property>

<property>
  <name>mapred.tasktracker.reduce.tasks.maximum</name>
  <value>1</value>
  <description>The maximum number of reduce tasks that will be run
  simultaneously by a task tracker.
  </description>
</property>

<property>
  <name>mapred.child.java.opts</name>
  <value>-Xmx1024m -Djava.net.preferIPv4Stack=true -XX:+UseParallelGC</value>
  <description>Java opts for the task tracker child processes.
  The following symbol, if present, will be interpolated: @taskid@ is replaced
  by current TaskID. Any other occurrences of '@' will go unchanged.
  For example, to enable verbose gc logging to a file named for the taskid in
  /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
        -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc

  The configuration variable mapred.child.ulimit can be used to control the
  maximum virtual memory of the child processes.
  </description>
</property>

  
<property>
  <name>mapred.compress.map.output</name>
  <value>true</value>
  <description>Should the outputs of the maps be compressed before being
               sent across the network. Uses SequenceFile compression.
  </description>
</property>
  
<property>
  <name>mapred.map.output.compression.codec</name>
  <value>org.apache.hadoop.io.compress.SnappyCodec</value>
  <description>If the map outputs are compressed, how should they be
               compressed?
  </description>
</property>

<!--  
<property>
  <name>mapred.output.compress</name>
  <value>true</value>
  <description>Should the job outputs be compressed?
  </description>
</property>

<property>
  <name>mapred.output.compression.type</name>
  <value>RECORD</value>
  <description>If the job outputs are to compressed as SequenceFiles, how should
               they be compressed? Should be one of NONE, RECORD or BLOCK.
  </description>
</property>

<property>
  <name>mapred.output.compression.codec</name>
  <value>org.apache.hadoop.io.compress.SnappyCodec</value>
  <description>If the job outputs are compressed, how should they be compressed?
  </description>
</property>

    -->         
<property>
  <name>mapreduce.tasktracker.outofband.heartbeat</name>
  <value>true</value>
</property>

<property>
  <name>topology.script.file.name</name>
  <value>/bi/app/hadoop-2.0.0-cdh4.3.0/etc/hadoop-mapreduce1/rack.py</value>
</property>

<!-- common conf END -->

 <property>
       <name>mapred.jobtracker.taskScheduler</name>
             <value>org.apache.hadoop.mapred.FairScheduler</value>
                    </property>
                           <property>
                                 <name>mapred.fairscheduler.allocation.file</name>
                                       <value>/bi/app/hadoop-2.0.0-cdh4.3.0/etc/hadoop-mapreduce1/fair-scheduler.xml</value>
                                              </property>
                                                      <property>
                                                           <name>mapred.fairscheduler.poolnameproperty</name>
                                                                 <value>pool.name</value>
                                                                        </property> 
</configuration>
