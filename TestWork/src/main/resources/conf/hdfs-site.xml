<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>

<!-- for namenode ha -->
<property>
  <name>dfs.nameservices</name>
  <value>bch</value>
</property>

<property>
  <name>dfs.ha.namenodes.bch</name>
  <value>nn1,nn2</value>
</property>

<property>
  <name>dfs.namenode.rpc-address.bch.nn1</name>
  <value>edc01:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.bch.nn2</name>
  <value>edc02:8020</value>
</property>

<property>
  <name>dfs.namenode.http-address.bch.nn1</name>
  <value>edc01:50070</value>
</property>
<property>
  <name>dfs.namenode.http-address.bch.nn2</name>
  <value>edc02:50070</value>
</property>

<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://edc01:8485;edc02:8485;edc03:8485/bch</value>
</property>

<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/home/hadoop/data/dfs/jn</value>
</property>

<property>
  <name>dfs.client.failover.proxy.provider.bch</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>

<property>
  <name>dfs.ha.fencing.methods</name>
  <value>shell(/bin/true)</value>
</property>
<!-- namenode ha end -->

<!-- namenode ha auto failover -->
<property>
  <name>dfs.ha.automatic-failover.enabled</name>
  <value>true</value>
</property>

<property>
  <name>ha.zookeeper.quorum</name>
  <value>edc01:2181,edc02:2181,edc03:2181</value>
</property>

<!-- namenode ha auto failover END-->

<property>
  <name>dfs.permissions.superusergroup</name>
  <value>hadoop</value>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>/home/hadoop/data/dfs/nn</value>
</property>

<property>
  <name>dfs.data.dir</name>
  <value>/data/hadoop/data/</value>
  <description>Determines where on the local filesystem an DFS data node
     should store its blocks.  If this is a comma-delimited
     list of directories, then data will be stored in all named
     directories, typically on different devices.
     Directories that do not exist are ignored.
  </description>
</property>

<property>
  <name>dfs.blocksize</name>
  <value>268435456</value>
</property>

<property>
  <name>dfs.datanode.failed.volumes.tolerated</name>
  <value>0</value>
</property>

<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
  <description>
    Enable WebHDFS (REST API) in Namenodes and Datanodes.
  </description>
</property>

<!-- config rpc timeout-->
<property>
  <name>ipc.client.ping</name>
  <value>false</value>
</property>
<property>
  <name>ipc.ping.interval</name>
  <value>60000</value>
</property>

<property>
  <name>topology.script.file.name</name>
  <value>etc/hadoop/rack.py</value>
</property>

<!--property>
       <name>dfs.name.edits.dir</name>
         <value>${hadoop.tmp.dir}/dfs/edits</value>
           <description>Determines where on the local filesystem the DFS name node
                 should store the transaction (edits) file. If this is a comma-delimited list
                       of directories then the transaction file is replicated in all of the
                             directories, for redundancy. Default value is same as dfs.name.dir
                               </description>
                               </property-->


<property>
  <name>dfs.replication</name>
    <value>2</value>
      <description>Default block replication.
        The actual number of replications can be specified when the file is created.
          The default is used if replication is not specified in create time.
            </description>
            </property>

<!--property>
       <name>dfs.web.ugi</name>
         <value>hadoop,hadoop</value>
           <description>The user account used by the web interface.
               Syntax: USERNAME,GROUP1,GROUP2, ...
                 </description>
                 </property-->

<!--property>
       <name>dfs.datanode.max.xcievers</name>
         <value>4096</value>
           <description>Under heavy read load, you may see lots of DFSClient complains about no live nodes hold a particular block.</description>
           </property-->

<property>
  <name>dfs.namenode.handler.count</name>
    <value>20</value>
      <description>The number of server threads for the namenode.</description>
      </property>

<property>
        <name>dfs.datanode.handler.count</name>
                <value>30</value>
                        <description>The number of server threads for the datanode.</description>
                        </property>

<property>
        <name>dfs.balance.bandwidthPerSec</name>
                <value>10485760</value>
                        <description>
                                        Specifies the maximum amount of bandwidth that each datanode
                                                        can utilize for the balancing purpose in term of
                                                                        the number of bytes per second.
                                                                                </description>
                                                                                </property>

<property>
        <name>dfs.permissions.enabled</name>
                <value>true</value>
                        <description>
                                        If "true", enable permission checking in HDFS.
                                                        If "false", permission checking is turned off,
                                                                        but all other behavior is unchanged.
                                                                                        Switching from one parameter value to the other does not change the mode,
                                                                                                        owner or group of files or directories.
                                                                                                                </description>
                                                                                                                </property>

<!--property>
             <name>dfs.hosts</name>
                     <value></value>
                             <description>Names a file that contains a list of hosts that are
                                             permitted to connect to the namenode. The full pathname of the file
                                                             must be specified.  If the value is empty, all hosts are
                                                                             permitted.
                                                                                     </description>
                                                                                     </property-->

<!--property>
             <name>dfs.hosts.exclude</name>
                     <value>/home/hadoop/bch/hadoop-0.20.2-cdh3u4/conf/nodes_exclude</value>
                             <description>Names a file that contains a list of hosts that are
                                             not permitted to connect to the namenode.  The full pathname of the
                                                             file must be specified.  If the value is empty, no hosts are
                                                                             excluded. At first, keep file 'nodes_exclude'  to be blank</description>
                                                                             </property-->

<property>
        <name>dfs.block.local-path-access.user</name>
                <value>hadoop</value>
                        <description>add users that need perform short circuit read here,datanode will do security check before the read.This has to be the user that started HBase.
                                </description>
                                </property>

<property>
        <name>dfs.client.read.shortcircuit</name>
                <value>true</value>
                        <description>set this to true to enable DFSClient short circuit</description>
                        </property>

<property>
        <name>dfs.client.read.shortcircuit.skip.checksum</name>
                <value>true</value>
                        <description>set this to true to enable DFSClient short circuit</description>
                        </property>
                        <property>
                            <name>dfs.domain.socket.path</name>
                                    <value>/var/run/hadoop_hdfs/dn._PORT</value>
                                            </property>
</configuration>
